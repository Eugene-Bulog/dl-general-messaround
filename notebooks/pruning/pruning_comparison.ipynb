{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small experiment to play with and compare some different pruning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import torch\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "import torch.utils.benchmark as benchmark\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "base_model_init = partial(resnet18, weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "control_model = base_model_init()\n",
    "control_model.to(device)\n",
    "\n",
    "models = {\"control\": control_model}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch's built in pruning support.\n",
    "Note that even their \"structured\" pruning still just zeros parameters, and doesn't actually remove the zeroed dimensions, so it won't have any actual effect on the speed or size of the model, just sparsifies it. We'd have to manually remove the zeroed elements to truly perform structural pruning and get the benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Built in pytorch pruning\n",
    "\n",
    "# local unstructured\n",
    "torch_local_unstruct_model = base_model_init()\n",
    "torch_local_unstruct_model.to(device)\n",
    "\n",
    "for _, module in torch_local_unstruct_model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        prune.random_unstructured(module, name=\"weight\", amount=0.3)\n",
    "        prune.remove(module, 'weight')\n",
    "\n",
    "models[\"torch_local_unstruct\"] = torch_local_unstruct_model\n",
    "\n",
    "# local structured\n",
    "torch_local_struct_model = base_model_init()\n",
    "torch_local_struct_model.to(device)\n",
    "\n",
    "for _, module in torch_local_struct_model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        prune.random_structured(module, name=\"weight\", amount=0.3, dim=0)\n",
    "        prune.remove(module, 'weight')\n",
    "\n",
    "models[\"torch_local_struct\"] = torch_local_struct_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_modelsize(models):\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        param_size = 0\n",
    "        for param in model.parameters():\n",
    "            param_size += param.nelement() * param.element_size()\n",
    "        buffer_size = 0\n",
    "        for buffer in model.buffers():\n",
    "            buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "        size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "        print(f\"\\\"{model_name}\\\" size: {size_all_mb:.3f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_benchmarks(models, batches=10, batch_size=16, channels=3, input_size=(224, 224)):\n",
    "    results = []\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    for _ in range(batches):\n",
    "        x = torch.randn(batch_size, channels, *input_size).to(device)\n",
    "\n",
    "        label = \"Model comparison\"\n",
    "        sub_label = \"Inference\"\n",
    "\n",
    "        for modelname, model in models.items():\n",
    "            results.append(benchmark.Timer(\n",
    "                stmt='model(x)',\n",
    "                globals={'x': x, 'model': model},\n",
    "                num_threads=1,\n",
    "                label=label,\n",
    "                sub_label=sub_label,\n",
    "                description=modelname\n",
    "            ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "    compare = benchmark.Compare(results)\n",
    "    compare.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------------------------- Model comparison ---------------------------]\n",
      "                 |  control  |  torch_local_unstruct  |  torch_local_struct\n",
      "1 threads: ----------------------------------------------------------------\n",
      "      Inference  |    8.6    |          8.6           |         8.6        \n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n",
      "\"control\" size: 44.629MB\n",
      "\"torch_local_unstruct\" size: 44.629MB\n",
      "\"torch_local_struct\" size: 44.629MB\n"
     ]
    }
   ],
   "source": [
    "compare_benchmarks(models)\n",
    "compare_modelsize(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As guessed, this doesn't actually provide any improved speed or size to the model, since the sparsified params aren't actually being sliced. Was still a useful exercise to get familiar with pytorch built in pruning methods and some benchmarking tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a third party lib. There isn't much in the way of fully auto solutions, I guess the most common use case is using something like pytorch's sparsification and then manually adding a step during training (or afterwards) to recreate the model with unneccessary nodes removed, and copy across weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch_pruning seems quite full-featured, and is still actively supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) => (conv1): Conv2d(3, 32, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) => (conv1): Conv2d(32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) => (conv2): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) => (conv1): Conv2d(32, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) => (conv2): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) => (conv1): Conv2d(32, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) => (conv2): Conv2d(56, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) => (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) => (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) => (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) => (conv1): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) => (conv2): Conv2d(96, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) => (0): Conv2d(64, 120, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) => (conv1): Conv2d(120, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) => (conv2): Conv2d(128, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) => (conv1): Conv2d(120, 248, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn1): BatchNorm2d(248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) => (conv2): Conv2d(248, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) => (0): Conv2d(120, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) => (conv1): Conv2d(256, 280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn1): BatchNorm2d(280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) => (conv2): Conv2d(280, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) => (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True) => (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
      ")\n",
      "\n",
      "MACs: 1.822177768 G -> 0.439866968 G, #Params: 11.689512 M -> 3.021512 M\n"
     ]
    }
   ],
   "source": [
    "import torch_pruning as tp\n",
    "\n",
    "high_level_tp_model = base_model_init()\n",
    "high_level_tp_model.to(device)\n",
    "\n",
    "example_inputs = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "imp = tp.importance.GroupMagnitudeImportance(p=2) \n",
    "ignored_layers = []\n",
    "for m in high_level_tp_model.modules():\n",
    "    if isinstance(m, torch.nn.Linear) and m.out_features == 1000:\n",
    "        ignored_layers.append(m)\n",
    "\n",
    "pruner = tp.pruner.BasePruner(\n",
    "    high_level_tp_model,\n",
    "    example_inputs,\n",
    "    importance=imp,\n",
    "    pruning_ratio=0.5, # remove 50% channels, ResNet18 = {64, 128, 256, 512} => ResNet18_Half = {32, 64, 128, 256}\n",
    "    # pruning_ratio_dict = {model.conv1: 0.2, model.layer2: 0.8}, # customized pruning ratios for layers or blocks\n",
    "    ignored_layers=ignored_layers,\n",
    "    round_to=8, # It's recommended to round dims/channels to 4x or 8x for acceleration. Please see: https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html\n",
    "    isomorphic=True, # enable isomorphic pruning to improve global ranking\n",
    "    global_pruning=True, # global pruning\n",
    ")\n",
    "\n",
    "base_macs, base_nparams = tp.utils.count_ops_and_params(high_level_tp_model, example_inputs)\n",
    "tp.utils.print_tool.before_pruning(high_level_tp_model) # or print(model)\n",
    "pruner.step()\n",
    "tp.utils.print_tool.after_pruning(high_level_tp_model) # or print(model), this util will show the difference before and after pruning\n",
    "macs, nparams = tp.utils.count_ops_and_params(high_level_tp_model, example_inputs)\n",
    "print(f\"MACs: {base_macs/1e9} G -> {macs/1e9} G, #Params: {base_nparams/1e6} M -> {nparams/1e6} M\")\n",
    "\n",
    "models[\"high_level_tp\"] = high_level_tp_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------- Model comparison ------------------------------------]\n",
      "                 |  control  |  torch_local_unstruct  |  torch_local_struct  |  high_level_tp\n",
      "1 threads: ----------------------------------------------------------------------------------\n",
      "      Inference  |    8.6    |          8.6           |         8.6          |       3.4     \n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n",
      "\"control\" size: 44.629MB\n",
      "\"torch_local_unstruct\" size: 44.629MB\n",
      "\"torch_local_struct\" size: 44.629MB\n",
      "\"high_level_tp\" size: 11.544MB\n"
     ]
    }
   ],
   "source": [
    "compare_benchmarks(models)\n",
    "compare_modelsize(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greatly improved model speed and size. Worth noting that when I tried this lib with the torchvision prebuilt ViT, it broke - possibly an issue in the weights of the transformer/attention blocks - if we add those to the ignore list it may work (although then the gains we'll get if we can't prune attention blocks will be a lot less)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
