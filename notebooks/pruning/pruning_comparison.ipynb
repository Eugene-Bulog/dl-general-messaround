{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small experiment to play with and compare some different pruning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_32, ViT_B_32_Weights\n",
    "import torch\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "import torch.utils.benchmark as benchmark\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "base_model_init = partial(vit_b_32, weights=ViT_B_32_Weights.IMAGENET1K_V1)\n",
    "\n",
    "control_model = base_model_init()\n",
    "control_model.to(device)\n",
    "\n",
    "models = {\"control\": control_model}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch's built in pruning support.\n",
    "Note that even their \"structured\" pruning still just zeros parameters, and doesn't actually remove the zeroed dimensions, so it won't have any actual effect on the speed or size of the model, just sparsifies it. We'd have to manually remove the zeroed elements to truly perform structural pruning and get the benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Built in pytorch pruning\n",
    "\n",
    "# local unstructured\n",
    "torch_local_unstruct_model = base_model_init()\n",
    "torch_local_unstruct_model.to(device)\n",
    "\n",
    "for _, module in torch_local_unstruct_model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        prune.random_unstructured(module, name=\"weight\", amount=0.3)\n",
    "        prune.remove(module, 'weight')\n",
    "\n",
    "models[\"torch_local_unstruct\"] = torch_local_unstruct_model\n",
    "\n",
    "# local structured\n",
    "torch_local_struct_model = base_model_init()\n",
    "torch_local_struct_model.to(device)\n",
    "\n",
    "for _, module in torch_local_struct_model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        prune.random_structured(module, name=\"weight\", amount=0.3, dim=0)\n",
    "        prune.remove(module, 'weight')\n",
    "\n",
    "models[\"torch_local_struct\"] = torch_local_struct_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_modelsize(models):\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model = control_model\n",
    "        param_size = 0\n",
    "        for param in model.parameters():\n",
    "            param_size += param.nelement() * param.element_size()\n",
    "        buffer_size = 0\n",
    "        for buffer in model.buffers():\n",
    "            buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "        size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "        print(f\"\\\"{model_name}\\\" size: {size_all_mb:.3f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_benchmarks(models, batches=10, batch_size=16, channels=3, input_size=(224, 224)):\n",
    "    results = []\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    for _ in range(batches):\n",
    "        x = torch.randn(batch_size, channels, *input_size).to(device)\n",
    "\n",
    "        label = \"Model comparison\"\n",
    "        sub_label = \"Inference\"\n",
    "\n",
    "        for modelname, model in models.items():\n",
    "            results.append(benchmark.Timer(\n",
    "                stmt='model(x)',\n",
    "                globals={'x': x, 'model': model},\n",
    "                num_threads=1,\n",
    "                label=label,\n",
    "                sub_label=sub_label,\n",
    "                description=modelname\n",
    "            ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "    compare = benchmark.Compare(results)\n",
    "    compare.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------------------------------------- Model comparison ---------------------------------------]\n",
      "                 |  control  |  torch_rand_unstruct  |  torch_local_unstruct  |  torch_local_struct\n",
      "1 threads: ----------------------------------------------------------------------------------------\n",
      "      Inference  |    22.0   |          23.8         |          22.1          |         22.1       \n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n",
      "\"control\" size: 336.549MB\n",
      "\"torch_rand_unstruct\" size: 336.549MB\n",
      "\"torch_local_unstruct\" size: 336.549MB\n",
      "\"torch_local_struct\" size: 336.549MB\n"
     ]
    }
   ],
   "source": [
    "compare_benchmarks(models)\n",
    "compare_modelsize(models)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As guessed, this doesn't actually provide any improved speed or size to the model, since the sparsified params aren't actually being sliced. Was still a useful exercise to get familiar with pytorch built in pruning methods and some benchmarking tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
